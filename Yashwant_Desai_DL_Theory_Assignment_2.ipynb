{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c27a818",
   "metadata": {},
   "source": [
    "# Yashwant Desai –  DL_Theory_Assignment_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0606451",
   "metadata": {},
   "source": [
    "# 1.\tDescribe the structure of an artificial neuron. How is it similar to a biological neuron? What are its main components?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d884bf",
   "metadata": {},
   "source": [
    "An artificial neuron also known as a perceptron is the fundamental building block of artificial neural networks. It's similar to a biological neuron in the sense that it receives inputs processes them, and produces an output. \n",
    "\n",
    "The main components of an artificial neuron include:\n",
    "\n",
    "Inputs (x1, x2, ..., xn): These are numerical values that represent the information or signals coming into the neuron.\n",
    "\n",
    "Weights (w1, w2, ..., wn): Each input is associated with a weight, which determines the significance of that input.\n",
    "\n",
    "Summation Function: The inputs are linearly combined with their corresponding weights usually represented as Σ(wi * xi).\n",
    "\n",
    "Activation Function: The summation result is passed through an activation function, which introduces non-linearity into the model and determines the neuron's output.\n",
    "\n",
    "Output (y): The activation function's output is the final result of the neuron's computation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39da1a19",
   "metadata": {},
   "source": [
    "# 2.\tWhat are the different types of activation functions popularly used? Explain each of them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50b3db2",
   "metadata": {},
   "source": [
    "Below are popular activation functions\n",
    "\n",
    "Step Function: It's a binary function where the output is 1 if the weighted sum of inputs is greater than a threshold, otherwise, it's 0.\n",
    "\n",
    "Sigmoid Function: S-shaped curve that maps the input to a value between 0 and 1. It's smooth and differentiable.\n",
    "\n",
    "Hyperbolic Tangent (tanh): Similar to the sigmoid but maps input to a value between -1 and 1, offering zero-centered outputs.\n",
    "\n",
    "Rectified Linear Unit (ReLU): It returns the input for positive values and zero for negative values, which is computationally efficient.\n",
    "\n",
    "Leaky Rectified Linear Unit (Leaky ReLU): Similar to ReLU, but it allows a small gradient for negative values, addressing the \"dying ReLU\" problem.\n",
    "\n",
    "Exponential Linear Unit (ELU): A variant of ReLU that has an exponential component for negative inputs, providing smooth gradients.\n",
    "\n",
    "Softmax Function: Typically used in the output layer of multi-class classification neural networks, it converts the input into a probability distribution over multiple classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd50488",
   "metadata": {},
   "source": [
    "# 3a.\tExplain, in details, Rosenblatt’s perceptron model. How can a set of data be classified using a simple perceptron?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec779dba",
   "metadata": {},
   "source": [
    "Rosenblatt's Perceptron Model: It's one of the earliest neural network models. The perceptron takes a set of inputs, multiplies them by their corresponding weights, and then applies an activation function often the step function. If the result is above a threshold it outputs one class; otherwise, it outputs the other class.\n",
    "\n",
    "Classifying Data Using a Simple Perceptron:\n",
    "\n",
    "To classify data using a simple perceptron you set the weights and the threshold. For example, with weights w0 = -1, w1 = 2, and w2 = 1, and a threshold of 0:\n",
    "\n",
    "Input (3, 4): Σ(wi * xi) = (-1 * 1) + (2 * 3) + (1 * 4) = 9 > 0, so it's classified as one class.\n",
    "\n",
    "Input (5, 2): Σ(wi * xi) = (-1 * 1) + (2 * 5) + (1 * 2) = 9 > 0, one class.\n",
    "\n",
    "Input (1, -3): Σ(wi * xi) = (-1 * 1) + (2 * 1) + (1 * -3) = -2 < 0, the other class.\n",
    "\n",
    "Input (-8, -3): Σ(wi * xi) = (-1 * 1) + (2 * -8) + (1 * -3) = -20 < 0, the other class.\n",
    "\n",
    "Input (-3, 0): Σ(wi * xi) = (-1 * 1) + (2 * -3) + (1 * 0) = -7 < 0, the other class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757df370",
   "metadata": {},
   "source": [
    "# 3b.\tUse a simple perceptron with weights w0, w1, and w2 as −1, 2, and 1, respectively, to classify data points (3, 4); (5, 2); (1, −3); (−8, −3); (−3, 0)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3685a4e4",
   "metadata": {},
   "source": [
    "We have a simple perceptron with three weights: w0 = -1, w1 = 2, and w2 = 1. We use this perceptron to classify different points based on their (x1, x2) coordinates.\n",
    "\n",
    "For the point (3, 4) when we put these coordinates into our perceptron we get a result of 6, which is greater than 0. So we classify this point as one class.\n",
    "\n",
    "Similarly for the point (5, 2), we get a result of 11 which is also greater than 0 so we classify it as the same class as the previous point.\n",
    "\n",
    "On the other hand for the point (1, -3) our perceptron gives us a result of -2 which is less than 0. Therefore, we classify this point as the other class.\n",
    "\n",
    "Point (-8, -3) results in -20, which is less than 0 so it's also the other class.\n",
    "\n",
    "Finally, for the point (-3, 0) our perceptron gives us -7 which is again less than 0 so it's classified as the other class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6058530",
   "metadata": {},
   "source": [
    "# 4.\tExplain the basic structure of a multi-layer perceptron. Explain how it can solve the XOR problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52cc9746",
   "metadata": {},
   "source": [
    "A multi-layer perceptron (MLP) consists of an input layer one or more hidden layers and an output layer. Each layer contains multiple neurons. The connections between neurons have associated weights and an activation function is applied at each neuron.\n",
    "\n",
    "The XOR problem can be solved using an MLP with at least one hidden layer. The XOR function is not linearly separable, meaning a single-layer perceptron can't solve it. However an MLP can learn to represent non-linear relationships. In the case of XOR a two-layer (one hidden layer) MLP can model the XOR function by learning the correct combinations of weights and activation functions to produce the desired output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93451aa",
   "metadata": {},
   "source": [
    "# 5.\tWhat is artificial neural network (ANN)? Explain some of the salient highlights in the different architectural options for ANN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f9ad6c",
   "metadata": {},
   "source": [
    "An Artificial Neural Network (ANN) is a computational model inspired by the structure and functioning of the human brain. It consists of interconnected artificial neurons (nodes) organized into layers.\n",
    "\n",
    "Salient highlights in different architectural options for ANN include:\n",
    "\n",
    "Feedforward Neural Network (FNN): Information flows in one direction from input to output.\n",
    "\n",
    "Recurrent Neural Network (RNN): Contains cycles in the connections, allowing it to model sequences and time-dependent data.\n",
    "\n",
    "Convolutional Neural Network (CNN): Designed for image and spatial data, uses convolutional layers to automatically learn \n",
    "features.\n",
    "\n",
    "Long Short-Term Memory (LSTM): A type of RNN with specialized memory cells for handling long-term dependencies.\n",
    "\n",
    "Gated Recurrent Unit (GRU): Similar to LSTM but computationally more efficient.\n",
    "\n",
    "Autoencoders: Used for dimensionality reduction and feature learning.\n",
    "Radial Basis Function (RBF) Networks: Employ radial basis functions as activation functions, often used in function approximation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf5f117",
   "metadata": {},
   "source": [
    "# 6.\tExplain the learning process of an ANN. Explain, with example, the challenge in assigning synaptic weights for the interconnection between neurons? How can this challenge be addressed?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf249d40",
   "metadata": {},
   "source": [
    "The learning process in an ANN involves adjusting synaptic weights to minimize the error between predicted and actual outputs. This is typically done through backpropagation and gradient descent algorithms.\n",
    "\n",
    "Example of Weight Assignment Challenge:\n",
    "Suppose we have an ANN for image recognition and one neuron in the output layer is responsible for identifying cats. Initially the network doesn't recognize cats well. The challenge is to adjust the weights to increase the probability of detecting cats while minimizing errors.\n",
    "\n",
    "This challenge can be addressed through iterative training where the network is exposed to a large dataset of labeled images and the weights are updated based on the error signal (the difference between predicted and actual labels) through backpropagation. The weights are updated in such a way that the error decreases leading to better recognition of cats."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d35b5a",
   "metadata": {},
   "source": [
    "# 7.\tExplain, in details, the backpropagation algorithm. What are the limitations of this algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d03e222",
   "metadata": {},
   "source": [
    "Backpropagation Algorithm: Backpropagation is a supervised learning algorithm used to train ANNs. \n",
    "It involves two phases: the forward pass where inputs are processed through the network to make predictions and the backward pass where errors are propagated backward to adjust the weights.\n",
    "\n",
    "Forward Pass: Calculate the predicted output of the network.\n",
    "\n",
    "Backward Pass: Calculate the error at the output layer and then propagate it backward through the network to adjust the weights using the gradient of the error with respect to the weights.\n",
    "\n",
    "Limitations of Backpropagation:\n",
    "\n",
    "It can get stuck in local minima.\n",
    "\n",
    "It may suffer from the vanishing gradient problem especially in deep networks.\n",
    "\n",
    "It doesn't always generalize well to unseen data.\n",
    "\n",
    "It requires a large amount of labeled training data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af98e770",
   "metadata": {},
   "source": [
    "# 8.\tDescribe, in details, the process of adjusting the interconnection weights in a multi-layer neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef0fb37",
   "metadata": {},
   "source": [
    "The process of adjusting weights in a multi-layer neural network, as part of training, involves the following steps:\n",
    "\n",
    "Initialization: Initialize the weights with small random values.\n",
    "\n",
    "Forward Pass: Pass the training data through the network to make predictions.\n",
    "\n",
    "Error Calculation: Calculate the error by comparing the predicted output to the actual target.\n",
    "\n",
    "Backpropagation: Propagate the error backward through the network layer by layer.\n",
    "\n",
    "Gradient Calculation: Calculate the gradient of the error with respect to each weight.\n",
    "\n",
    "Weight Update: Update the weights in the direction that reduces the error typically using gradient descent or its variants like Adam, RMSProp, or stochastic gradient descent.\n",
    "\n",
    "Repeat: Iterate through the training data for a fixed number of epochs or until convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaee7f87",
   "metadata": {},
   "source": [
    "# 9.\tWhat are the steps in the backpropagation algorithm? Why a multi-layer neural network is required?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d54e69b",
   "metadata": {},
   "source": [
    "Steps in the backpropagation algorithm:\n",
    "\n",
    "Initialize the weights.\n",
    "\n",
    "Perform a forward pass to compute the network's output.\n",
    "\n",
    "Calculate the error between the predicted and actual output.\n",
    "\n",
    "Perform a backward pass to calculate gradients of the error with respect to each weight.\n",
    "\n",
    "Update the weights to minimize the error using gradient descent.\n",
    "\n",
    "A multi-layer neural network is required because it can model complex non-linear relationships in data. Single-layer networks (perceptrons) are only capable of linear separability which limits their ability to solve complex problems. Multi-layer networks with hidden layers introduce non-linearity and can approximate a wide range of functions making them suitable for more challenging tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd9d568",
   "metadata": {},
   "source": [
    "# 10.\tWrite short notes on:\n",
    "1.\tArtificial neuron\n",
    "2.\tMulti-layer perceptron\n",
    "3.\tDeep learning\n",
    "4.\tLearning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd31cce",
   "metadata": {},
   "source": [
    "Below are short notes on Artificial neuron , Multi-layer perceptron, Deep learning & Learning rate\n",
    "\n",
    "Artificial Neuron: An artificial neuron or perceptron, is the basic building block of artificial neural networks. It receives input applies weights, performs a weighted sum and passes the result through an activation function to produce an output.\n",
    "\n",
    "Multi-layer Perceptron: A multi-layer perceptron is a type of feedforward neural network with an input layer one or more hidden layers and an output layer. It can model complex non-linear relationships and is used for various machine learning tasks.\n",
    "\n",
    "Deep Learning: Deep learning is a subset of machine learning that uses deep neural networks with multiple hidden layers. It's particularly effective in tasks like image and speech recognition.\n",
    "\n",
    "Learning Rate: Learning rate is a hyperparameter in training neural networks. It controls the step size in weight updates during gradient descent. A higher learning rate can lead to faster convergence but it may cause overshooting while a lower learning rate can slow down convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe17079",
   "metadata": {},
   "source": [
    "# 11.\tWrite the difference between:-\n",
    "1.\tActivation function vs threshold function\n",
    "2.\tStep function vs sigmoid function\n",
    "3.\tSingle layer vs multi-layer perceptron\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643672a5",
   "metadata": {},
   "source": [
    "Below are differences between:- Activation function vs threshold function, Step function vs sigmoid function & Single layer vs multi-layer perceptron\n",
    "\n",
    "Activation function vs. Threshold function: Activation functions introduce non-linearity and vary continuously, while threshold functions are binary, having a fixed threshold for output.\n",
    "\n",
    "Step function vs. Sigmoid function: Step functions are binary, with a sudden change in output at a threshold, while the sigmoid function is smooth and continuously differentiable, producing values between 0 and 1.\n",
    "\n",
    "Single layer vs. Multi-layer perceptron: A single layer perceptron can only model linearly separable problems, while a multi-layer perceptron can model non-linearly separable problems, making it more versatile for complex tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda7d5b1",
   "metadata": {},
   "source": [
    "# Done all 11 questions "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04469f9",
   "metadata": {},
   "source": [
    "# Regards,Yashwant"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
